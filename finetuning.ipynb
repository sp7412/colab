{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO0uWZ8hFAYXI/nXTg1uDTw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sp7412/colab/blob/master/finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-Ds-Nlth5v0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "be83dca7-9b20-4971-a4b3-f87553605078"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jul 24 15:58:41 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.51.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19glg_B7j00M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "86af7d0a-994d-4611-a5b1-34d471271381"
      },
      "source": [
        "!nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name, driver_version, memory.total [MiB]\n",
            "Tesla K80, 418.67, 11441 MiB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZMyWRcub4AP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "f75f1808-231a-4a1a-d492-cd48927c06bd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjlYcHZ30uTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers,models\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG6uE3FmnxTa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a3f7b4d5-5abf-4928-c7d8-4701fcfbc74d"
      },
      "source": [
        "tf.config.list_physical_devices('GPU')[0]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLpmq0-S0uQT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Preprocessing functions from data_util.py in SimCLR repository (hidden).\n",
        "\n",
        "FLAGS_color_jitter_strength = 0.3\n",
        "CROP_PROPORTION = 0.875  # Standard for ImageNet.\n",
        "\n",
        "\n",
        "def random_apply(func, p, x):\n",
        "  \"\"\"Randomly apply function func to x with probability p.\"\"\"\n",
        "  return tf.cond(\n",
        "      tf.less(tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
        "              tf.cast(p, tf.float32)),\n",
        "      lambda: func(x),\n",
        "      lambda: x)\n",
        "\n",
        "\n",
        "def random_brightness(image, max_delta, impl='simclrv2'):\n",
        "  \"\"\"A multiplicative vs additive change of brightness.\"\"\"\n",
        "  if impl == 'simclrv2':\n",
        "    factor = tf.random_uniform(\n",
        "        [], tf.maximum(1.0 - max_delta, 0), 1.0 + max_delta)\n",
        "    image = image * factor\n",
        "  elif impl == 'simclrv1':\n",
        "    image = random_brightness(image, max_delta=max_delta)\n",
        "  else:\n",
        "    raise ValueError('Unknown impl {} for random brightness.'.format(impl))\n",
        "  return image\n",
        "\n",
        "\n",
        "def to_grayscale(image, keep_channels=True):\n",
        "  image = tf.image.rgb_to_grayscale(image)\n",
        "  if keep_channels:\n",
        "    image = tf.tile(image, [1, 1, 3])\n",
        "  return image\n",
        "\n",
        "\n",
        "def color_jitter(image,\n",
        "                 strength,\n",
        "                 random_order=True):\n",
        "  \"\"\"Distorts the color of the image.\n",
        "  Args:\n",
        "    image: The input image tensor.\n",
        "    strength: the floating number for the strength of the color augmentation.\n",
        "    random_order: A bool, specifying whether to randomize the jittering order.\n",
        "  Returns:\n",
        "    The distorted image tensor.\n",
        "  \"\"\"\n",
        "  brightness = 0.8 * strength\n",
        "  contrast = 0.8 * strength\n",
        "  saturation = 0.8 * strength\n",
        "  hue = 0.2 * strength\n",
        "  if random_order:\n",
        "    return color_jitter_rand(image, brightness, contrast, saturation, hue)\n",
        "  else:\n",
        "    return color_jitter_nonrand(image, brightness, contrast, saturation, hue)\n",
        "\n",
        "\n",
        "def color_jitter_nonrand(image, brightness=0, contrast=0, saturation=0, hue=0):\n",
        "  \"\"\"Distorts the color of the image (jittering order is fixed).\n",
        "  Args:\n",
        "    image: The input image tensor.\n",
        "    brightness: A float, specifying the brightness for color jitter.\n",
        "    contrast: A float, specifying the contrast for color jitter.\n",
        "    saturation: A float, specifying the saturation for color jitter.\n",
        "    hue: A float, specifying the hue for color jitter.\n",
        "  Returns:\n",
        "    The distorted image tensor.\n",
        "  \"\"\"\n",
        "  with tf.name_scope('distort_color'):\n",
        "    def apply_transform(i, x, brightness, contrast, saturation, hue):\n",
        "      \"\"\"Apply the i-th transformation.\"\"\"\n",
        "      if brightness != 0 and i == 0:\n",
        "        x = random_brightness(x, max_delta=brightness)\n",
        "      elif contrast != 0 and i == 1:\n",
        "        x = tf.image.random_contrast(\n",
        "            x, lower=1-contrast, upper=1+contrast)\n",
        "      elif saturation != 0 and i == 2:\n",
        "        x = tf.image.random_saturation(\n",
        "            x, lower=1-saturation, upper=1+saturation)\n",
        "      elif hue != 0:\n",
        "        x = tf.image.random_hue(x, max_delta=hue)\n",
        "      return x\n",
        "\n",
        "    for i in range(4):\n",
        "      image = apply_transform(i, image, brightness, contrast, saturation, hue)\n",
        "      image = tf.clip_by_value(image, 0., 1.)\n",
        "    return image\n",
        "\n",
        "\n",
        "def color_jitter_rand(image, brightness=0, contrast=0, saturation=0, hue=0):\n",
        "  \"\"\"Distorts the color of the image (jittering order is random).\n",
        "  Args:\n",
        "    image: The input image tensor.\n",
        "    brightness: A float, specifying the brightness for color jitter.\n",
        "    contrast: A float, specifying the contrast for color jitter.\n",
        "    saturation: A float, specifying the saturation for color jitter.\n",
        "    hue: A float, specifying the hue for color jitter.\n",
        "  Returns:\n",
        "    The distorted image tensor.\n",
        "  \"\"\"\n",
        "  with tf.name_scope('distort_color'):\n",
        "    def apply_transform(i, x):\n",
        "      \"\"\"Apply the i-th transformation.\"\"\"\n",
        "      def brightness_foo():\n",
        "        if brightness == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return random_brightness(x, max_delta=brightness)\n",
        "      def contrast_foo():\n",
        "        if contrast == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return tf.image.random_contrast(x, lower=1-contrast, upper=1+contrast)\n",
        "      def saturation_foo():\n",
        "        if saturation == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return tf.image.random_saturation(\n",
        "              x, lower=1-saturation, upper=1+saturation)\n",
        "      def hue_foo():\n",
        "        if hue == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return tf.image.random_hue(x, max_delta=hue)\n",
        "      x = tf.cond(tf.less(i, 2),\n",
        "                  lambda: tf.cond(tf.less(i, 1), brightness_foo, contrast_foo),\n",
        "                  lambda: tf.cond(tf.less(i, 3), saturation_foo, hue_foo))\n",
        "      return x\n",
        "\n",
        "    perm = tf.random_shuffle(tf.range(4))\n",
        "    for i in range(4):\n",
        "      image = apply_transform(perm[i], image)\n",
        "      image = tf.clip_by_value(image, 0., 1.)\n",
        "    return image\n",
        "\n",
        "\n",
        "def _compute_crop_shape(\n",
        "    image_height, image_width, aspect_ratio, crop_proportion):\n",
        "  \"\"\"Compute aspect ratio-preserving shape for central crop.\n",
        "  The resulting shape retains `crop_proportion` along one side and a proportion\n",
        "  less than or equal to `crop_proportion` along the other side.\n",
        "  Args:\n",
        "    image_height: Height of image to be cropped.\n",
        "    image_width: Width of image to be cropped.\n",
        "    aspect_ratio: Desired aspect ratio (width / height) of output.\n",
        "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
        "  Returns:\n",
        "    crop_height: Height of image after cropping.\n",
        "    crop_width: Width of image after cropping.\n",
        "  \"\"\"\n",
        "  image_width_float = tf.cast(image_width, tf.float32)\n",
        "  image_height_float = tf.cast(image_height, tf.float32)\n",
        "\n",
        "  def _requested_aspect_ratio_wider_than_image():\n",
        "    crop_height = tf.cast(tf.math.rint(\n",
        "        crop_proportion / aspect_ratio * image_width_float), tf.int32)\n",
        "    crop_width = tf.cast(tf.math.rint(\n",
        "        crop_proportion * image_width_float), tf.int32)\n",
        "    return crop_height, crop_width\n",
        "\n",
        "  def _image_wider_than_requested_aspect_ratio():\n",
        "    crop_height = tf.cast(\n",
        "        tf.math.rint(crop_proportion * image_height_float), tf.int32)\n",
        "    crop_width = tf.cast(tf.math.rint(\n",
        "        crop_proportion * aspect_ratio *\n",
        "        image_height_float), tf.int32)\n",
        "    return crop_height, crop_width\n",
        "\n",
        "  return tf.cond(\n",
        "      aspect_ratio > image_width_float / image_height_float,\n",
        "      _requested_aspect_ratio_wider_than_image,\n",
        "      _image_wider_than_requested_aspect_ratio)\n",
        "\n",
        "\n",
        "def center_crop(image, height, width, crop_proportion):\n",
        "  \"\"\"Crops to center of image and rescales to desired size.\n",
        "  Args:\n",
        "    image: Image Tensor to crop.\n",
        "    height: Height of image to be cropped.\n",
        "    width: Width of image to be cropped.\n",
        "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
        "  Returns:\n",
        "    A `height` x `width` x channels Tensor holding a central crop of `image`.\n",
        "  \"\"\"\n",
        "  shape = tf.shape(image)\n",
        "  image_height = shape[0]\n",
        "  image_width = shape[1]\n",
        "  crop_height, crop_width = _compute_crop_shape(\n",
        "      image_height, image_width, height / width, crop_proportion)\n",
        "  offset_height = ((image_height - crop_height) + 1) // 2\n",
        "  offset_width = ((image_width - crop_width) + 1) // 2\n",
        "  image = tf.image.crop_to_bounding_box(\n",
        "      image, offset_height, offset_width, crop_height, crop_width)\n",
        "\n",
        "  image = tf.image.resize([image], [height, width])[0]\n",
        "\n",
        "  return image\n",
        "\n",
        "\n",
        "def distorted_bounding_box_crop(image,\n",
        "                                bbox,\n",
        "                                min_object_covered=0.1,\n",
        "                                aspect_ratio_range=(0.75, 1.33),\n",
        "                                area_range=(0.05, 1.0),\n",
        "                                max_attempts=100,\n",
        "                                scope=None):\n",
        "  \"\"\"Generates cropped_image using one of the bboxes randomly distorted.\n",
        "  See `tf.image.sample_distorted_bounding_box` for more documentation.\n",
        "  Args:\n",
        "    image: `Tensor` of image data.\n",
        "    bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n",
        "        where each coordinate is [0, 1) and the coordinates are arranged\n",
        "        as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n",
        "        image.\n",
        "    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n",
        "        area of the image must contain at least this fraction of any bounding\n",
        "        box supplied.\n",
        "    aspect_ratio_range: An optional list of `float`s. The cropped area of the\n",
        "        image must have an aspect ratio = width / height within this range.\n",
        "    area_range: An optional list of `float`s. The cropped area of the image\n",
        "        must contain a fraction of the supplied image within in this range.\n",
        "    max_attempts: An optional `int`. Number of attempts at generating a cropped\n",
        "        region of the image of the specified constraints. After `max_attempts`\n",
        "        failures, return the entire image.\n",
        "    scope: Optional `str` for name scope.\n",
        "  Returns:\n",
        "    (cropped image `Tensor`, distorted bbox `Tensor`).\n",
        "  \"\"\"\n",
        "  with tf.name_scope(scope, 'distorted_bounding_box_crop', [image, bbox]):\n",
        "    shape = tf.shape(image)\n",
        "    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n",
        "        shape,\n",
        "        bounding_boxes=bbox,\n",
        "        min_object_covered=min_object_covered,\n",
        "        aspect_ratio_range=aspect_ratio_range,\n",
        "        area_range=area_range,\n",
        "        max_attempts=max_attempts,\n",
        "        use_image_if_no_bounding_boxes=True)\n",
        "    bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n",
        "\n",
        "    # Crop the image to the specified bounding box.\n",
        "    offset_y, offset_x, _ = tf.unstack(bbox_begin)\n",
        "    target_height, target_width, _ = tf.unstack(bbox_size)\n",
        "    image = tf.image.crop_to_bounding_box(\n",
        "        image, offset_y, offset_x, target_height, target_width)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def crop_and_resize(image, height, width):\n",
        "  \"\"\"Make a random crop and resize it to height `height` and width `width`.\n",
        "  Args:\n",
        "    image: Tensor representing the image.\n",
        "    height: Desired image height.\n",
        "    width: Desired image width.\n",
        "  Returns:\n",
        "    A `height` x `width` x channels Tensor holding a random crop of `image`.\n",
        "  \"\"\"\n",
        "  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n",
        "  aspect_ratio = width / height\n",
        "  image = distorted_bounding_box_crop(\n",
        "      image,\n",
        "      bbox,\n",
        "      min_object_covered=0.1,\n",
        "      aspect_ratio_range=(3. / 4 * aspect_ratio, 4. / 3. * aspect_ratio),\n",
        "      area_range=(0.08, 1.0),\n",
        "      max_attempts=100,\n",
        "      scope=None)\n",
        "  return tf.image.resize([image], [height, width])[0]\n",
        "\n",
        "\n",
        "def gaussian_blur(image, kernel_size, sigma, padding='SAME'):\n",
        "  \"\"\"Blurs the given image with separable convolution.\n",
        "  Args:\n",
        "    image: Tensor of shape [height, width, channels] and dtype float to blur.\n",
        "    kernel_size: Integer Tensor for the size of the blur kernel. This is should\n",
        "      be an odd number. If it is an even number, the actual kernel size will be\n",
        "      size + 1.\n",
        "    sigma: Sigma value for gaussian operator.\n",
        "    padding: Padding to use for the convolution. Typically 'SAME' or 'VALID'.\n",
        "  Returns:\n",
        "    A Tensor representing the blurred image.\n",
        "  \"\"\"\n",
        "  radius = tf.to_int32(kernel_size / 2)\n",
        "  kernel_size = radius * 2 + 1\n",
        "  x = tf.to_float(tf.range(-radius, radius + 1))\n",
        "  blur_filter = tf.exp(\n",
        "      -tf.pow(x, 2.0) / (2.0 * tf.pow(tf.to_float(sigma), 2.0)))\n",
        "  blur_filter /= tf.reduce_sum(blur_filter)\n",
        "  # One vertical and one horizontal filter.\n",
        "  blur_v = tf.reshape(blur_filter, [kernel_size, 1, 1, 1])\n",
        "  blur_h = tf.reshape(blur_filter, [1, kernel_size, 1, 1])\n",
        "  num_channels = tf.shape(image)[-1]\n",
        "  blur_h = tf.tile(blur_h, [1, 1, num_channels, 1])\n",
        "  blur_v = tf.tile(blur_v, [1, 1, num_channels, 1])\n",
        "  expand_batch_dim = image.shape.ndims == 3\n",
        "  if expand_batch_dim:\n",
        "    # Tensorflow requires batched input to convolutions, which we can fake with\n",
        "    # an extra dimension.\n",
        "    image = tf.expand_dims(image, axis=0)\n",
        "  blurred = tf.nn.depthwise_conv2d(\n",
        "      image, blur_h, strides=[1, 1, 1, 1], padding=padding)\n",
        "  blurred = tf.nn.depthwise_conv2d(\n",
        "      blurred, blur_v, strides=[1, 1, 1, 1], padding=padding)\n",
        "  if expand_batch_dim:\n",
        "    blurred = tf.squeeze(blurred, axis=0)\n",
        "  return blurred\n",
        "\n",
        "\n",
        "def random_crop_with_resize(image, height, width, p=1.0):\n",
        "  \"\"\"Randomly crop and resize an image.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    p: Probability of applying this transformation.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  def _transform(image):  # pylint: disable=missing-docstring\n",
        "    image = crop_and_resize(image, height, width)\n",
        "    return image\n",
        "  return random_apply(_transform, p=p, x=image)\n",
        "\n",
        "\n",
        "def random_color_jitter(image, p=1.0):\n",
        "  def _transform(image):\n",
        "    color_jitter_t = functools.partial(\n",
        "        color_jitter, strength=FLAGS_color_jitter_strength)\n",
        "    image = random_apply(color_jitter_t, p=0.8, x=image)\n",
        "    return random_apply(to_grayscale, p=0.2, x=image)\n",
        "  return random_apply(_transform, p=p, x=image)\n",
        "\n",
        "\n",
        "def random_blur(image, height, width, p=1.0):\n",
        "  \"\"\"Randomly blur an image.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    p: probability of applying this transformation.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  del width\n",
        "  def _transform(image):\n",
        "    sigma = tf.random.uniform([], 0.1, 2.0, dtype=tf.float32)\n",
        "    return gaussian_blur(\n",
        "        image, kernel_size=height//10, sigma=sigma, padding='SAME')\n",
        "  return random_apply(_transform, p=p, x=image)\n",
        "\n",
        "\n",
        "def batch_random_blur(images_list, height, width, blur_probability=0.5):\n",
        "  \"\"\"Apply efficient batch data transformations.\n",
        "  Args:\n",
        "    images_list: a list of image tensors.\n",
        "    height: the height of image.\n",
        "    width: the width of image.\n",
        "    blur_probability: the probaility to apply the blur operator.\n",
        "  Returns:\n",
        "    Preprocessed feature list.\n",
        "  \"\"\"\n",
        "  def generate_selector(p, bsz):\n",
        "    shape = [bsz, 1, 1, 1]\n",
        "    selector = tf.cast(\n",
        "        tf.less(tf.random_uniform(shape, 0, 1, dtype=tf.float32), p),\n",
        "        tf.float32)\n",
        "    return selector\n",
        "\n",
        "  new_images_list = []\n",
        "  for images in images_list:\n",
        "    images_new = random_blur(images, height, width, p=1.)\n",
        "    selector = generate_selector(blur_probability, tf.shape(images)[0])\n",
        "    images = images_new * selector + images * (1 - selector)\n",
        "    images = tf.clip_by_value(images, 0., 1.)\n",
        "    new_images_list.append(images)\n",
        "\n",
        "  return new_images_list\n",
        "\n",
        "\n",
        "def preprocess_for_train(image, height, width,\n",
        "                         color_distort=True, crop=True, flip=True):\n",
        "  \"\"\"Preprocesses the given image for training.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    color_distort: Whether to apply the color distortion.\n",
        "    crop: Whether to crop the image.\n",
        "    flip: Whether or not to flip left and right of an image.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  if crop:\n",
        "    image = random_crop_with_resize(image, height, width)\n",
        "  if flip:\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "  if color_distort:\n",
        "    image = random_color_jitter(image)\n",
        "  image = tf.reshape(image, [height, width, 3])\n",
        "  image = tf.clip_by_value(image, 0., 1.)\n",
        "  return image\n",
        "\n",
        "\n",
        "def preprocess_for_eval(image, height, width, crop=True):\n",
        "  \"\"\"Preprocesses the given image for evaluation.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    crop: Whether or not to (center) crop the test images.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  if crop:\n",
        "    image = center_crop(image, height, width, crop_proportion=CROP_PROPORTION)\n",
        "  image = tf.reshape(image, [height, width, 3])\n",
        "  image = tf.clip_by_value(image, 0., 1.)\n",
        "  return image\n",
        "\n",
        "\n",
        "def preprocess_image(image, height, width, is_training=False,\n",
        "                     color_distort=True, test_crop=True):\n",
        "  \"\"\"Preprocesses the given image.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    is_training: `bool` for whether the preprocessing is for training.\n",
        "    color_distort: whether to apply the color distortion.\n",
        "    test_crop: whether or not to extract a central crop of the images\n",
        "        (as for standard ImageNet evaluation) during the evaluation.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor` of range [0, 1].\n",
        "  \"\"\"\n",
        "  image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "  if is_training:\n",
        "    return preprocess_for_train(image, height, width, color_distort)\n",
        "  else:\n",
        "    return preprocess_for_eval(image, height, width, test_crop)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QwpHQVpmA7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = tf.keras.utils.get_file(\n",
        "    'flower_photos',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "    untar=True)\n",
        "\n",
        "batch_size = 64\n",
        "dataset_name = 'tf_flowers'\n",
        "\n",
        "tfds_dataset, tfds_info = tfds.load(\n",
        "    dataset_name, split='train', with_info=True)\n",
        "num_images = tfds_info.splits['train'].num_examples\n",
        "num_classes = tfds_info.features['label'].num_classes\n",
        "\n",
        "def _preprocess(x):\n",
        "  x['image'] = preprocess_image(\n",
        "      x['image'], 224, 224, is_training=False, color_distort=False)\n",
        "  return x\n",
        "\n",
        "x = tfds_dataset.map(_preprocess).batch(batch_size)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG-SZUgJQzNA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0042ddc8-769f-48f2-fd9f-37ff8147a209"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = (224,224)\n",
        "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\n",
        "dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=BATCH_SIZE,\n",
        "                   interpolation=\"bilinear\")\n",
        "\n",
        "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    **datagen_kwargs)\n",
        "valid_generator = valid_datagen.flow_from_directory(\n",
        "    data_dir, subset=\"validation\", shuffle=False, **dataflow_kwargs)\n",
        "\n",
        "do_data_augmentation = False #@param {type:\"boolean\"}\n",
        "if do_data_augmentation:\n",
        "  train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "      rotation_range=40,\n",
        "      horizontal_flip=True,\n",
        "      width_shift_range=0.2, height_shift_range=0.2,\n",
        "      shear_range=0.2, zoom_range=0.2,\n",
        "      **datagen_kwargs)\n",
        "else:\n",
        "  train_datagen = valid_datagen\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir, subset=\"training\", shuffle=True, **dataflow_kwargs)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 731 images belonging to 5 classes.\n",
            "Found 2939 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5EBIYcFdsG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -l \"/content/drive/My Drive/20200723-202620resnet_simclr.h5\"\n",
        "!ls -l \"/content/drive/My Drive/20200723-213702resnet_simclr_model.h5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N3R8Yicb603",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saved_model_path = \"/content/drive/My Drive/20200723-213702resnet_simclr_model.h5\""
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCW0efwycEdy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0fa2963e-2591-4588-cb57-85f9252500ac"
      },
      "source": [
        "saved_model = tf.keras.models.load_model(saved_model_path)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBBXGKX3iaRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saved_model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3q6MJD0swO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saved_model.trainable = False"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StvWu82JqWg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert([layer.trainable == False for layer in saved_model.layers])"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivXzz_A6xoYY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "ce14295b-aa72-40fe-bb1a-450b7bd45bd2"
      },
      "source": [
        "saved_model.summary()"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "resnet50 (Model)             (None, 7, 7, 2048)        23587712  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               524544    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                6450      \n",
            "=================================================================\n",
            "Total params: 24,151,602\n",
            "Trainable params: 24,098,482\n",
            "Non-trainable params: 53,120\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct6lT8SKtKKu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "e29617e0-4920-411b-e2fc-87f848690733"
      },
      "source": [
        "for idx, layer in enumerate(saved_model.layers):\n",
        "    print('layer[{}].name = {}'.format(idx, layer.name, saved_model.layers[idx]))"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "layer[0].name = input_2\n",
            "layer[1].name = resnet50\n",
            "layer[2].name = global_average_pooling2d\n",
            "layer[3].name = dense\n",
            "layer[4].name = activation\n",
            "layer[5].name = dense_1\n",
            "layer[6].name = activation_1\n",
            "layer[7].name = dense_2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivbiU4_50vL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx, layer in enumerate(saved_model.layers):\n",
        "    layer.trainable = False"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pv30q960FbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saved_model.layers[-1].trainable = True"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IEYT8jtz210",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saved_model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nme9enWmzwXB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "059af8bc-0f63-4e1a-edaa-a61dbf657e3b"
      },
      "source": [
        "saved_model.summary()"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "resnet50 (Model)             (None, 7, 7, 2048)        23587712  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               524544    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                6450      \n",
            "=================================================================\n",
            "Total params: 24,151,602\n",
            "Trainable params: 6,450\n",
            "Non-trainable params: 24,145,152\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0L4_-_giSCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_layer = layers.Dense(num_classes, activation='softmax', name='scores')(saved_model.output)"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VejISEiQxOuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Model(saved_model.input, output_layer)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hbC6OhVx0j4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.build((None,)+IMAGE_SIZE+(3,))"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hruPeWm-yHcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMOxJdf4zLpa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "52788bc9-df39-423f-db72-20086451aa7c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "resnet50 (Model)             (None, 7, 7, 2048)        23587712  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               524544    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                6450      \n",
            "_________________________________________________________________\n",
            "scores (Dense)               (None, 5)                 255       \n",
            "=================================================================\n",
            "Total params: 24,151,857\n",
            "Trainable params: 6,705\n",
            "Non-trainable params: 24,145,152\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhy9oIIY44vN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "5aa9b737-cc45-4fb6-fbc0-6cd132eecb30"
      },
      "source": [
        "for idx, layer in enumerate(model.layers):\n",
        "    print(idx, layer.name, layer.trainable)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 input_2 False\n",
            "1 resnet50 False\n",
            "2 global_average_pooling2d False\n",
            "3 dense False\n",
            "4 activation False\n",
            "5 dense_1 False\n",
            "6 activation_1 False\n",
            "7 dense_2 True\n",
            "8 scores True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99ZruzOPQ-hZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "outputId": "afba9454-9fb4-48fe-8dda-db1cd4857780"
      },
      "source": [
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "validation_steps = valid_generator.samples // valid_generator.batch_size\n",
        "hist = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20, steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=valid_generator,\n",
        "    validation_steps=validation_steps).history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "91/91 [==============================] - 28s 306ms/step - loss: 2.1581 - accuracy: 0.2938 - val_loss: 1.8535 - val_accuracy: 0.3040\n",
            "Epoch 2/20\n",
            "91/91 [==============================] - 27s 294ms/step - loss: 1.5845 - accuracy: 0.3464 - val_loss: 1.6266 - val_accuracy: 0.2955\n",
            "Epoch 3/20\n",
            "91/91 [==============================] - 27s 293ms/step - loss: 1.5093 - accuracy: 0.3770 - val_loss: 1.5774 - val_accuracy: 0.3509\n",
            "Epoch 4/20\n",
            "91/91 [==============================] - 27s 293ms/step - loss: 1.4724 - accuracy: 0.3891 - val_loss: 1.6620 - val_accuracy: 0.3224\n",
            "Epoch 5/20\n",
            "91/91 [==============================] - 27s 292ms/step - loss: 1.4645 - accuracy: 0.3963 - val_loss: 1.5241 - val_accuracy: 0.3395\n",
            "Epoch 6/20\n",
            "91/91 [==============================] - 27s 292ms/step - loss: 1.4430 - accuracy: 0.3887 - val_loss: 1.5103 - val_accuracy: 0.3693\n",
            "Epoch 7/20\n",
            "91/91 [==============================] - 27s 292ms/step - loss: 1.4244 - accuracy: 0.4032 - val_loss: 1.5826 - val_accuracy: 0.3438\n",
            "Epoch 8/20\n",
            "91/91 [==============================] - 27s 292ms/step - loss: 1.4361 - accuracy: 0.3983 - val_loss: 1.4733 - val_accuracy: 0.3764\n",
            "Epoch 9/20\n",
            "91/91 [==============================] - 27s 291ms/step - loss: 1.4061 - accuracy: 0.4169 - val_loss: 1.4804 - val_accuracy: 0.3565\n",
            "Epoch 10/20\n",
            "91/91 [==============================] - 27s 292ms/step - loss: 1.4054 - accuracy: 0.4114 - val_loss: 1.4539 - val_accuracy: 0.3821\n",
            "Epoch 11/20\n",
            "91/91 [==============================] - 27s 292ms/step - loss: 1.4002 - accuracy: 0.4063 - val_loss: 1.4551 - val_accuracy: 0.3835\n",
            "Epoch 12/20\n",
            "91/91 [==============================] - 27s 292ms/step - loss: 1.3995 - accuracy: 0.4245 - val_loss: 1.5790 - val_accuracy: 0.3310\n",
            "Epoch 13/20\n",
            "91/91 [==============================] - 27s 292ms/step - loss: 1.4018 - accuracy: 0.4180 - val_loss: 1.4724 - val_accuracy: 0.3736\n",
            "Epoch 14/20\n",
            "91/91 [==============================] - 27s 292ms/step - loss: 1.4188 - accuracy: 0.4162 - val_loss: 1.5314 - val_accuracy: 0.3523\n",
            "Epoch 15/20\n",
            "91/91 [==============================] - 27s 291ms/step - loss: 1.3993 - accuracy: 0.4204 - val_loss: 1.4827 - val_accuracy: 0.3523\n",
            "Epoch 16/20\n",
            "91/91 [==============================] - 27s 291ms/step - loss: 1.3957 - accuracy: 0.4221 - val_loss: 1.5070 - val_accuracy: 0.3835\n",
            "Epoch 17/20\n",
            "91/91 [==============================] - ETA: 0s - loss: 1.3932 - accuracy: 0.4166"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kptnbpm20uXJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a4a1621d-8268-427e-fae8-892b5381cd4a"
      },
      "source": [
        "hist = model.fit(\n",
        "    train_generator,\n",
        "    epochs=100, steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=valid_generator,\n",
        "    validation_steps=validation_steps).history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "91/91 [==============================] - 539s 6s/step - loss: 3.0218 - accuracy: 0.2466 - val_loss: 2.2411 - val_accuracy: 0.2997\n",
            "Epoch 2/100\n",
            "91/91 [==============================] - 541s 6s/step - loss: 2.0917 - accuracy: 0.3037 - val_loss: 1.9078 - val_accuracy: 0.3068\n",
            "Epoch 3/100\n",
            "91/91 [==============================] - 542s 6s/step - loss: 1.8398 - accuracy: 0.3261 - val_loss: 1.7969 - val_accuracy: 0.2983\n",
            "Epoch 4/100\n",
            "91/91 [==============================] - 539s 6s/step - loss: 1.7351 - accuracy: 0.3304 - val_loss: 1.7195 - val_accuracy: 0.3310\n",
            "Epoch 5/100\n",
            "91/91 [==============================] - 541s 6s/step - loss: 1.6627 - accuracy: 0.3388 - val_loss: 1.6824 - val_accuracy: 0.3011\n",
            "Epoch 6/100\n",
            "91/91 [==============================] - 541s 6s/step - loss: 1.6144 - accuracy: 0.3416 - val_loss: 1.6458 - val_accuracy: 0.3111\n",
            "Epoch 7/100\n",
            "91/91 [==============================] - 540s 6s/step - loss: 1.5883 - accuracy: 0.3481 - val_loss: 1.6193 - val_accuracy: 0.3210\n",
            "Epoch 8/100\n",
            "91/91 [==============================] - 538s 6s/step - loss: 1.5593 - accuracy: 0.3529 - val_loss: 1.6032 - val_accuracy: 0.3267\n",
            "Epoch 9/100\n",
            "91/91 [==============================] - 542s 6s/step - loss: 1.5380 - accuracy: 0.3609 - val_loss: 1.5843 - val_accuracy: 0.3324\n",
            "Epoch 10/100\n",
            "91/91 [==============================] - 543s 6s/step - loss: 1.5222 - accuracy: 0.3557 - val_loss: 1.5784 - val_accuracy: 0.3324\n",
            "Epoch 11/100\n",
            "91/91 [==============================] - 533s 6s/step - loss: 1.5149 - accuracy: 0.3667 - val_loss: 1.5692 - val_accuracy: 0.3281\n",
            "Epoch 12/100\n",
            "91/91 [==============================] - 534s 6s/step - loss: 1.4997 - accuracy: 0.3681 - val_loss: 1.5563 - val_accuracy: 0.3452\n",
            "Epoch 13/100\n",
            "91/91 [==============================] - 534s 6s/step - loss: 1.4912 - accuracy: 0.3639 - val_loss: 1.5432 - val_accuracy: 0.3409\n",
            "Epoch 14/100\n",
            "91/91 [==============================] - 534s 6s/step - loss: 1.4827 - accuracy: 0.3729 - val_loss: 1.5411 - val_accuracy: 0.3395\n",
            "Epoch 15/100\n",
            "91/91 [==============================] - 536s 6s/step - loss: 1.4751 - accuracy: 0.3698 - val_loss: 1.5292 - val_accuracy: 0.3494\n",
            "Epoch 16/100\n",
            "91/91 [==============================] - 537s 6s/step - loss: 1.4654 - accuracy: 0.3770 - val_loss: 1.5297 - val_accuracy: 0.3409\n",
            "Epoch 17/100\n",
            "91/91 [==============================] - 538s 6s/step - loss: 1.4650 - accuracy: 0.3818 - val_loss: 1.5312 - val_accuracy: 0.3494\n",
            "Epoch 18/100\n",
            "91/91 [==============================] - 538s 6s/step - loss: 1.4601 - accuracy: 0.3798 - val_loss: 1.5169 - val_accuracy: 0.3580\n",
            "Epoch 19/100\n",
            "91/91 [==============================] - 539s 6s/step - loss: 1.4497 - accuracy: 0.3770 - val_loss: 1.5342 - val_accuracy: 0.3523\n",
            "Epoch 20/100\n",
            "91/91 [==============================] - 535s 6s/step - loss: 1.4500 - accuracy: 0.3822 - val_loss: 1.5131 - val_accuracy: 0.3580\n",
            "Epoch 21/100\n",
            "91/91 [==============================] - 539s 6s/step - loss: 1.4451 - accuracy: 0.3853 - val_loss: 1.5101 - val_accuracy: 0.3622\n",
            "Epoch 22/100\n",
            "91/91 [==============================] - 543s 6s/step - loss: 1.4436 - accuracy: 0.3908 - val_loss: 1.5091 - val_accuracy: 0.3665\n",
            "Epoch 23/100\n",
            "91/91 [==============================] - 538s 6s/step - loss: 1.4401 - accuracy: 0.3877 - val_loss: 1.5064 - val_accuracy: 0.3636\n",
            "Epoch 24/100\n",
            "91/91 [==============================] - 536s 6s/step - loss: 1.4355 - accuracy: 0.3963 - val_loss: 1.5082 - val_accuracy: 0.3494\n",
            "Epoch 25/100\n",
            "91/91 [==============================] - 532s 6s/step - loss: 1.4349 - accuracy: 0.3891 - val_loss: 1.4971 - val_accuracy: 0.3636\n",
            "Epoch 26/100\n",
            "91/91 [==============================] - 535s 6s/step - loss: 1.4318 - accuracy: 0.3897 - val_loss: 1.4988 - val_accuracy: 0.3707\n",
            "Epoch 27/100\n",
            "91/91 [==============================] - 536s 6s/step - loss: 1.4292 - accuracy: 0.3946 - val_loss: 1.4978 - val_accuracy: 0.3537\n",
            "Epoch 28/100\n",
            "91/91 [==============================] - 533s 6s/step - loss: 1.4264 - accuracy: 0.3935 - val_loss: 1.4977 - val_accuracy: 0.3636\n",
            "Epoch 29/100\n",
            "91/91 [==============================] - 549s 6s/step - loss: 1.4288 - accuracy: 0.3915 - val_loss: 1.4977 - val_accuracy: 0.3736\n",
            "Epoch 30/100\n",
            "91/91 [==============================] - 542s 6s/step - loss: 1.4205 - accuracy: 0.3966 - val_loss: 1.4866 - val_accuracy: 0.3594\n",
            "Epoch 31/100\n",
            "91/91 [==============================] - 536s 6s/step - loss: 1.4220 - accuracy: 0.3925 - val_loss: 1.4909 - val_accuracy: 0.3622\n",
            "Epoch 32/100\n",
            "91/91 [==============================] - 537s 6s/step - loss: 1.4242 - accuracy: 0.3956 - val_loss: 1.4931 - val_accuracy: 0.3722\n",
            "Epoch 33/100\n",
            "91/91 [==============================] - 535s 6s/step - loss: 1.4179 - accuracy: 0.3970 - val_loss: 1.4836 - val_accuracy: 0.3679\n",
            "Epoch 34/100\n",
            "91/91 [==============================] - 539s 6s/step - loss: 1.4180 - accuracy: 0.3987 - val_loss: 1.4817 - val_accuracy: 0.3750\n",
            "Epoch 35/100\n",
            "91/91 [==============================] - 546s 6s/step - loss: 1.4187 - accuracy: 0.3980 - val_loss: 1.4853 - val_accuracy: 0.3665\n",
            "Epoch 36/100\n",
            "91/91 [==============================] - 533s 6s/step - loss: 1.4169 - accuracy: 0.3997 - val_loss: 1.4894 - val_accuracy: 0.3651\n",
            "Epoch 37/100\n",
            "91/91 [==============================] - 533s 6s/step - loss: 1.4159 - accuracy: 0.4014 - val_loss: 1.4857 - val_accuracy: 0.3580\n",
            "Epoch 38/100\n",
            "91/91 [==============================] - 531s 6s/step - loss: 1.4187 - accuracy: 0.4028 - val_loss: 1.4778 - val_accuracy: 0.3736\n",
            "Epoch 39/100\n",
            "91/91 [==============================] - 537s 6s/step - loss: 1.4119 - accuracy: 0.4014 - val_loss: 1.4857 - val_accuracy: 0.3679\n",
            "Epoch 40/100\n",
            "91/91 [==============================] - 544s 6s/step - loss: 1.4133 - accuracy: 0.4100 - val_loss: 1.4774 - val_accuracy: 0.3636\n",
            "Epoch 41/100\n",
            "91/91 [==============================] - 559s 6s/step - loss: 1.4126 - accuracy: 0.4121 - val_loss: 1.4743 - val_accuracy: 0.3778\n",
            "Epoch 42/100\n",
            "91/91 [==============================] - 559s 6s/step - loss: 1.4130 - accuracy: 0.4018 - val_loss: 1.4751 - val_accuracy: 0.3679\n",
            "Epoch 43/100\n",
            "91/91 [==============================] - 550s 6s/step - loss: 1.4062 - accuracy: 0.4104 - val_loss: 1.4734 - val_accuracy: 0.3722\n",
            "Epoch 44/100\n",
            "91/91 [==============================] - 537s 6s/step - loss: 1.4098 - accuracy: 0.4063 - val_loss: 1.4739 - val_accuracy: 0.3622\n",
            "Epoch 45/100\n",
            "91/91 [==============================] - 533s 6s/step - loss: 1.4084 - accuracy: 0.4149 - val_loss: 1.4807 - val_accuracy: 0.3636\n",
            "Epoch 46/100\n",
            "91/91 [==============================] - ETA: 0s - loss: 1.4076 - accuracy: 0.4045"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIWySH0Bnfen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}